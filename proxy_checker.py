import streamlit as st
import asyncio
import aiohttp
from aiohttp_socks import ProxyConnector
import pandas as pd
import time
import datetime
from streamlit_gsheets import GSheetsConnection

# 1. é¡µé¢åŸºæœ¬é…ç½®
st.set_page_config(page_title="å“ˆå¯†ç“œç§‘æŠ€ - ç§æœ‰æ£€æµ‹å·¥å…·", layout="wide")

st.title("ğŸ›¡ï¸ SOCKS5 ä»£ç†æ‰¹é‡æ£€æµ‹")
st.caption("è¯·æŒ‰ä»¥ä¸‹æç¤ºçš„æ ¼å¼å¡«å†™:72.1.133.228:7620:user:passã€‚")

# 2. åˆå§‹åŒ– Google Sheets è¿æ¥
conn = st.connection("gsheets", type=GSheetsConnection)

# --- æ ¸å¿ƒè§£æé€»è¾‘ ---
def parse_proxy(proxy_str):
    p = proxy_str.strip()
    if not p: return None
    # æ”¯æŒ IP:Port:User:Pass æ ¼å¼
    parts = p.split(':')
    if len(parts) == 4:
        ip, port, user, pwd = parts
        return f"socks5://{user}:{pwd}@{ip}:{port}"
    # æ”¯æŒ User:Pass@IP:Port æˆ– IP:Port æ ¼å¼
    return f"socks5://{p}"

async def fetch_ip_info(session):
    """è·å– IP åœ°ç†ä½ç½®ä¿¡æ¯"""
    try:
        async with session.get("http://ip-api.com/json/?lang=zh-CN", timeout=10) as resp:
            if resp.status == 200:
                return await resp.json()
    except:
        return None
    return None

async def check_single_proxy(raw_proxy, semaphore, test_url, timeout):
    """æ£€æµ‹å•ä¸ªä»£ç†"""
    async with semaphore:
        formatted_url = parse_proxy(raw_proxy)
        if not formatted_url: return None
        
        start_time = time.time()
        try:
            connector = ProxyConnector.from_url(formatted_url)
            async with aiohttp.ClientSession(connector=connector) as session:
                # 1. è·å–åœ°ç†ä½ç½® (åŒæ—¶ä¹Ÿè¯æ˜äº†ä»£ç†æ˜¯é€šçš„)
                info = await fetch_ip_info(session)
                latency = int((time.time() - start_time) * 1000)
                
                if info and info.get("status") == "success":
                    return {
                        "åŸå§‹åœ°å€": raw_proxy,
                        "çŠ¶æ€": "âœ… æˆåŠŸ",
                        "å»¶è¿Ÿ": f"{latency}ms",
                        "å‡ºå£ IP": info.get("query"),
                        "å›½å®¶/åœ°åŒº": f"{info.get('country')} - {info.get('city')}",
                        "è¿è¥å•†": info.get("isp")
                    }
        except:
            pass
        return {"åŸå§‹åœ°å€": raw_proxy, "çŠ¶æ€": "âŒ å¤±è´¥", "å»¶è¿Ÿ": "-", "å‡ºå£ IP": "-", "å›½å®¶/åœ°åŒº": "-", "è¿è¥å•†": "-"}

async def run_checks(proxies, max_concurrency, test_url, timeout):
    """æ‰¹é‡è¿è¡Œå¼‚æ­¥æ£€æµ‹"""
    semaphore = asyncio.Semaphore(max_concurrency)
    tasks = [check_single_proxy(p, semaphore, test_url, timeout) for p in proxies]
    return await asyncio.gather(*tasks)

# --- ä¾§è¾¹æ è®¾ç½® ---
with st.sidebar:
    st.header("âš™ï¸ æ£€æµ‹é…ç½®")
    test_url = st.text_input("æµ‹è¯•åœ°å€", value="http://www.google.com/generate_204")
    timeout = st.slider("è¶…æ—¶æ—¶é—´ (ç§’)", 1, 30, 15)
    max_c = st.number_input("å¹¶å‘çº¿ç¨‹æ•°", 1, 100, 20)
    st.divider()
    st.write("åˆ¶ä½œå•ä½ï¼šå“ˆå¯†ç“œç§‘æŠ€")

# --- ä¸»ç•Œé¢ ---
input_text = st.text_area("è¯·è¾“å…¥ä»£ç†åˆ—è¡¨ (æ¯è¡Œä¸€ä¸ª)", placeholder="72.1.133.228:7620:user:pass", height=200)

if st.button("ğŸš€ å¼€å§‹æ‰¹é‡æ£€æµ‹å¹¶å»é‡åŒæ­¥", type="primary"):
    proxies = [p.strip() for p in input_text.split('\n') if p.strip()]
    
    if not proxies:
        st.warning("è¯·å…ˆè¾“å…¥ä»£ç†åœ°å€åˆ—è¡¨ï¼")
    else:
        with st.spinner("æ­£åœ¨æ£€æµ‹ä¸­ï¼Œè¯·ç¨å€™..."):
            # A. æ‰§è¡Œæ£€æµ‹ä»»åŠ¡
            results = asyncio.run(run_checks(proxies, max_c, test_url, timeout))
            df_new = pd.DataFrame(results)
            
            # B. æå–æˆåŠŸçš„ç»“æœ
            success_df = df_new[df_new["çŠ¶æ€"] == "âœ… æˆåŠŸ"].copy()
            
            if not success_df.empty:
                # æ·»åŠ å½“å‰æ—¶é—´æˆ³
                success_df["ä¿å­˜æ—¶é—´"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                try:
                    # C. è¯»å– Google Sheets ç°æœ‰æ•°æ®
                    existing_df = conn.read().dropna(how="all")
                    
                    # D. åˆå¹¶æ•°æ®å¹¶å»é‡
                    # subset=['åŸå§‹åœ°å€'] è¡¨ç¤ºæ ¹æ®ä»£ç†åœ°å€åˆ¤æ–­é‡å¤
                    # keep='last' è¡¨ç¤ºå¦‚æœæœ‰é‡å¤ï¼Œä¿ç•™æ–°æ£€æµ‹åˆ°çš„è¿™ä¸€æ¡
                    combined_df = pd.concat([existing_df, success_df], ignore_index=True)
                    final_df = combined_df.drop_duplicates(subset=['åŸå§‹åœ°å€'], keep='last')
                    
                    # E. å†™å› Google Sheets
                    conn.update(data=final_df)
                    st.toast("æ•°æ®åŒæ­¥æˆåŠŸï¼å·²è‡ªåŠ¨å»é‡ã€‚", icon="âœ…")
                except Exception as e:
                    st.error(f"åŒæ­¥è‡³ Google Sheets å¤±è´¥ï¼š{e}")
            
            # F. å±•ç¤ºæœ¬æ¬¡æ£€æµ‹ç»“æœ
            st.success(f"æ£€æµ‹å®Œæˆï¼æœ¬æ¬¡æˆåŠŸï¼š{len(success_df)} / æ€»æ•°ï¼š{len(proxies)}")
            st.dataframe(df_new, use_container_width=True)
            
            # æä¾›æœ¬åœ°ä¸‹è½½
            csv = df_new.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è½½æœ¬æ¬¡æ£€æµ‹æŠ¥å‘Š (CSV)", csv, "proxy_results.csv", "text/csv")